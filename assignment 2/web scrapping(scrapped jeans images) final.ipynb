{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (3.141.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: urllib3 in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from selenium) (1.25.9)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import StaleElementReferenceException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common import exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (3.2.1)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from webdriver-manager) (2.24.0)\n",
      "Requirement already satisfied: crayons in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from webdriver-manager) (0.3.1)\n",
      "Requirement already satisfied: configparser in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from webdriver-manager) (5.0.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests->webdriver-manager) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests->webdriver-manager) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests->webdriver-manager) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests->webdriver-manager) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\comp\\.conda\\envs\\tensorflow\\lib\\site-packages (from crayons->webdriver-manager) (0.4.3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 83.0.4103\n",
      "[WDM] - Get LATEST driver version for 83.0.4103\n",
      "[WDM] - Driver [C:\\Users\\COMP\\.wdm\\drivers\\chromedriver\\win32\\83.0.4103.39\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "DRIVER_PATH = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "#and this opens up my chrome controlled by my driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=DRIVER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page_url=driver.get('https://www.flipkart.com/clothing-and-accessories/bottomwear/jeans/men-jeans/pr?sid=clo,vua,k58,i51&otracker=categorytree&otracker=nmenu_sub_Men_0_Jeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(dirname):\n",
    "    current_path=os.getcwd()\n",
    "    path=os.path.join(current_path,dirname)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME=\"men_jeans_scrapped\"\n",
    "make_directory(DIRNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_image_url(driver):\n",
    "    images=driver.find_elements_by_xpath(\"//img[@class='_3togXc']\")\n",
    "    product_data={}\n",
    "    product_data['image_urls']=[]\n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['image_urls'].append(source)\n",
    "    print(\"returning scrapped data\")\n",
    "    return product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(data,dirname,page):\n",
    "    for index, link in enumerate(data['image_urls']):\n",
    "        print(\"downloading {0} of {1} images\".format(index+ 1, len(data['image_urls'])))\n",
    "        response=requests.get(link)\n",
    "        with open('{0}/img_{1}{2}.jpeg'.format(dirname,page,index),\"wb\")as file:\n",
    "            file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_page=1\n",
    "total_pages=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping 3 pages each page has 40 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returning scrapped data\n",
      "scrapping page 1 of 3 pages\n",
      "the current page scrapped is 1\n",
      "downloading 1 of 40 images\n",
      "downloading 2 of 40 images\n",
      "downloading 3 of 40 images\n",
      "downloading 4 of 40 images\n",
      "downloading 5 of 40 images\n",
      "downloading 6 of 40 images\n",
      "downloading 7 of 40 images\n",
      "downloading 8 of 40 images\n",
      "downloading 9 of 40 images\n",
      "downloading 10 of 40 images\n",
      "downloading 11 of 40 images\n",
      "downloading 12 of 40 images\n",
      "downloading 13 of 40 images\n",
      "downloading 14 of 40 images\n",
      "downloading 15 of 40 images\n",
      "downloading 16 of 40 images\n",
      "downloading 17 of 40 images\n",
      "downloading 18 of 40 images\n",
      "downloading 19 of 40 images\n",
      "downloading 20 of 40 images\n",
      "downloading 21 of 40 images\n",
      "downloading 22 of 40 images\n",
      "downloading 23 of 40 images\n",
      "downloading 24 of 40 images\n",
      "downloading 25 of 40 images\n",
      "downloading 26 of 40 images\n",
      "downloading 27 of 40 images\n",
      "downloading 28 of 40 images\n",
      "downloading 29 of 40 images\n",
      "downloading 30 of 40 images\n",
      "downloading 31 of 40 images\n",
      "downloading 32 of 40 images\n",
      "downloading 33 of 40 images\n",
      "downloading 34 of 40 images\n",
      "downloading 35 of 40 images\n",
      "downloading 36 of 40 images\n",
      "downloading 37 of 40 images\n",
      "downloading 38 of 40 images\n",
      "downloading 39 of 40 images\n",
      "downloading 40 of 40 images\n",
      "scrapping of page 1 done\n",
      "moving to the next page\n",
      "the new page is 1\n",
      "returning scrapped data\n",
      "scrapping page 2 of 3 pages\n",
      "the current page scrapped is 2\n",
      "downloading 1 of 40 images\n",
      "downloading 2 of 40 images\n",
      "downloading 3 of 40 images\n",
      "downloading 4 of 40 images\n",
      "downloading 5 of 40 images\n",
      "downloading 6 of 40 images\n",
      "downloading 7 of 40 images\n",
      "downloading 8 of 40 images\n",
      "downloading 9 of 40 images\n",
      "downloading 10 of 40 images\n",
      "downloading 11 of 40 images\n",
      "downloading 12 of 40 images\n",
      "downloading 13 of 40 images\n",
      "downloading 14 of 40 images\n",
      "downloading 15 of 40 images\n",
      "downloading 16 of 40 images\n",
      "downloading 17 of 40 images\n",
      "downloading 18 of 40 images\n",
      "downloading 19 of 40 images\n",
      "downloading 20 of 40 images\n",
      "downloading 21 of 40 images\n",
      "downloading 22 of 40 images\n",
      "downloading 23 of 40 images\n",
      "downloading 24 of 40 images\n",
      "downloading 25 of 40 images\n",
      "downloading 26 of 40 images\n",
      "downloading 27 of 40 images\n",
      "downloading 28 of 40 images\n",
      "downloading 29 of 40 images\n",
      "downloading 30 of 40 images\n",
      "downloading 31 of 40 images\n",
      "downloading 32 of 40 images\n",
      "downloading 33 of 40 images\n",
      "downloading 34 of 40 images\n",
      "downloading 35 of 40 images\n",
      "downloading 36 of 40 images\n",
      "downloading 37 of 40 images\n",
      "downloading 38 of 40 images\n",
      "downloading 39 of 40 images\n",
      "downloading 40 of 40 images\n",
      "scrapping of page 2 done\n",
      "moving to the next page\n",
      "the new page is 2\n",
      "returning scrapped data\n",
      "scrapping page 3 of 3 pages\n",
      "the current page scrapped is 3\n",
      "downloading 1 of 40 images\n",
      "downloading 2 of 40 images\n",
      "downloading 3 of 40 images\n",
      "downloading 4 of 40 images\n",
      "downloading 5 of 40 images\n",
      "downloading 6 of 40 images\n",
      "downloading 7 of 40 images\n",
      "downloading 8 of 40 images\n",
      "downloading 9 of 40 images\n",
      "downloading 10 of 40 images\n",
      "downloading 11 of 40 images\n",
      "downloading 12 of 40 images\n",
      "downloading 13 of 40 images\n",
      "downloading 14 of 40 images\n",
      "downloading 15 of 40 images\n",
      "downloading 16 of 40 images\n",
      "downloading 17 of 40 images\n",
      "downloading 18 of 40 images\n",
      "downloading 19 of 40 images\n",
      "downloading 20 of 40 images\n",
      "downloading 21 of 40 images\n",
      "downloading 22 of 40 images\n",
      "downloading 23 of 40 images\n",
      "downloading 24 of 40 images\n",
      "downloading 25 of 40 images\n",
      "downloading 26 of 40 images\n",
      "downloading 27 of 40 images\n",
      "downloading 28 of 40 images\n",
      "downloading 29 of 40 images\n",
      "downloading 30 of 40 images\n",
      "downloading 31 of 40 images\n",
      "downloading 32 of 40 images\n",
      "downloading 33 of 40 images\n",
      "downloading 34 of 40 images\n",
      "downloading 35 of 40 images\n",
      "downloading 36 of 40 images\n",
      "downloading 37 of 40 images\n",
      "downloading 38 of 40 images\n",
      "downloading 39 of 40 images\n",
      "downloading 40 of 40 images\n",
      "scrapping of page 3 done\n",
      "moving to the next page\n",
      "the new page is 3\n"
     ]
    }
   ],
   "source": [
    "for page in range(start_page,total_pages+1):\n",
    "    try:\n",
    "        product_details=scrap_image_url(driver=driver)\n",
    "        print(\"scrapping page {0} of {1} pages\".format(page,total_pages))\n",
    "        page_value=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"the current page scrapped is {}\".format(page_value))\n",
    "        save_images(data=product_details,dirname=DIRNAME,page=page)\n",
    "        print(\"scrapping of page {0} done\".format(page))\n",
    "        print(\"moving to the next page\")\n",
    "        button_type=driver.find_element_by_xpath(\"//div[@class='_2zg3yZ']//a[@class='_3fVaIS']//span\").get_attribute('innerHTML')\n",
    "        if button_type=='Next':\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS']\").click()\n",
    "        else:\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS'][2]\").click()\n",
    "            \n",
    "        new_page=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"the new page is {}\".format(new_page))\n",
    "    except StaleElementReferenceException as Exception:\n",
    "        print(\"we are facing an exception\")\n",
    "        exp_page=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"the page value at the time of exception is {}\".format(exp_page))\n",
    "        value=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\")\n",
    "        link=value.get_attribute('href')\n",
    "        driver.get(link)\n",
    "        product_details=scrap_image_url(driver=driver)\n",
    "        print(\"scrapping page {0} of {1} pages\".format(page,total_pages))\n",
    "        page_value=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"the current page scrapped is {}\".format(page_value))\n",
    "        save_images(data=product_details,dirname=DIRNAME,page=page)\n",
    "        print(\"scrapping of page {0} done\".format(page))\n",
    "        print(\"moving to the next page\")\n",
    "        button_type=driver.find_element_by_xpath(\"//div[@class='_2zg3yZ']//a[@class='_3fVaIS']//span\").get_attribute('innerHTML')\n",
    "        if button_type=='Next':\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS']\").click()\n",
    "        else:\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS'][2]\").click()\n",
    "            new_page=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "            print(\"the new page is {}\".format(new_page))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
